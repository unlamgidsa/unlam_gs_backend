{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attrition_Bancario - Drive.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KSLgqOSknCdK"},"source":["# TRABAJO PRACTICO: Modelo de predicción de Attrition Bancario"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TtHK4DH0nCdM"},"source":["## Objetivo\n","El objetivo de este trabajo es construir un modelo que permita predecir qué clientes se van a ir del banco el mes próximo."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cX0VJhqVnCdN"},"source":["Recursos Adicionales\n","\n","- [scikit-learn documentación de árboles de decisión](http://scikit-learn.org/stable/modules/tree.html)\n","- [Gini Vs Entropia](http://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0M7wfK1DnCdP"},"source":["## 1. Análisis descriptivo del dataset\n","\n","El set de entrenamiento a utilizar es **dataset_training.csv** el cual contiene información sobre 60903 clientes (personas físicas) con paquetes \"Premium\" de una entidad bancaria. En el archivo **DiccionarioDatos-TP.xls** se especifica la descripción de cada uno de sus 36 atributos y algunas aclaraciones sobre la información suministrada.\n","\n","Instrucciones:\n","\n","1. Instalar todas las librerias python que aquí se incluyen.\n","2. Instalar graph visualizer en windows: https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n","3. Leer los datos con Pandas.\n","   - Explorar los datos ordenándolos o graficándolos."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_7hH3gmMnCdQ","colab":{}},"source":["# Librerias a importar\n","import numpy as np\n","import pandas as pd\n","import pydotplus\n","import seaborn as sns\n","import copy\n","import matplotlib.pyplot as plt\n","\n","# Por defecto, matplotlib crea una figura en una ventana separada. Cuando usamos Jupyter notebooks, podemos hacer que las \n","# figuras aparezcan en línea dentro del notebook. Esto lo hacemos ejecutando: \n","%matplotlib inline \n","\n","from IPython.display import Image\n","\n","# Scikit-learn (sklearn) es una librería que implementa algunos algoritmos de Machine Learning y pre-procesamiento de datos.\n","\n","from sklearn.tree import export_graphviz  \n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics.scorer import make_scorer\n","from sklearn import metrics\n","from sklearn.svm import SVR\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import LabelEncoder\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"79dQ8E5Tx2Vf","colab":{}},"source":["# Leemos el dataset de entrenamiento\n","dataset_entrenamiento='dataset_training.csv'\n","df = pd.read_csv(dataset_entrenamiento, sep =';', na_values = '.', index_col = 0)\n","df.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"INR7F23-PsbR","colab":{}},"source":["# ¿Qué información estadística obtenemos del dataset?\n","df.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SLjeVnAcPsbW","colab":{}},"source":["# Veamos la distribución de clientes que se dieron de baja de los que continuan\n","print(df['CLASE'].value_counts(normalize = True)*100) # expresado en porcentajes\n","print(df['CLASE'].value_counts(normalize = False)) # expresado en nominal\n","sns.countplot(x='CLASE', data=df)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"10Ic8AqaPsbc"},"source":["### 1.a) Análisis de correlación de variables\n"," \n","Una matriz de correlación permite estudiar la relación lineal o comportamiento que puede existir entre dos o más variables.\n","\n","  - Correlación positiva: ocurre cuando una variable aumenta y la otra también.\n","  - Correlación negativa: es cuando una variable aumenta y la otra disminuye. \n","  - Sin correlación: no hay una relación aparente entre las variables. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n4Y5ZBoBPsbd","colab":{}},"source":["# Creamos una variable numerica CLASE_NUM a partir de nuestra variable objetivo CLASE. \n","# La misma tendrá un valor 1 cuando el cliente sea una BAJA y un 0 cuando CONTINUA como cliente. \n","# Esta variable se usará para calcular la correlacion lineal entre las variables númericas y la variable objetivo.\n","\n","df['CLASE_NUM'] = list(map(lambda clase: 1 if (clase == 'BAJA') else 0, df['CLASE']))\n","\n","# Calculamos la matriz de correlacion entre todas las variables del dataset. \n","df.corr()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2KgUeesBPsbh","colab":{}},"source":["# Seleccionamos sólo la correlacion de la variable objetivo numerica CLASE_NUM.\n","dfd = df.corr()[[\"CLASE_NUM\"]]*100\n","dfd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"53pE18v3Psbk","colab":{}},"source":["# Borramos la correlacion de la variable objetivo numerica consigo misma.\n","dfd = dfd.drop(\"CLASE_NUM\", axis=0)   # CLASE_NUM = 100\n","\n","# Ordenamos las variables de forma decreciente por el valor de correlacion positiva con la variable objetivo.\n","dfd = dfd.sort_values([\"CLASE_NUM\"], ascending=False)\n","dfd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IbERS1_SPsbo","colab":{}},"source":["# Graficamos el mapa de calor.\n","plt.figure(figsize=(10, 20))\n","sns.heatmap(dfd, robust=True, linewidths=.5, annot=True, )\n","\n","# Para terminar borramos la variable numérica creada\n","df.drop('CLASE_NUM', axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_48hvZiSPsbu"},"source":["### Análisis preliminar\n","\n","Del mapa de calor se puede observar que existe una alta correlación positiva entre las variables VISA_MARCA_ATRASO y MASTER_MARCA_ATRASO respecto a la posibilidad que un cliente se de baja. Al parecer, esto indicaría que aquellos clientes que no realizan el pago mínimo de las tarjetas o están atrasados en los pagos son más propensos a darse de baja. Las restantes variables con correlación positiva tendrán relación con la observación preliminar realizada? "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fIslVmLxPsbv"},"source":["### 1.b) Análisis de la edad y antigüedad de un cliente\n","\n","Vamos a analizar ahora cómo afecta la edad y antigüedad del cliente con la posibilidad a darse de baja."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z4vzC5DxPsbw","colab":{}},"source":["# Creamos un gráfico boxplot para analizar la EDAD del cliente respecto a la CLASE\n","plt.figure(figsize=(4, 8))\n","s=sns.boxplot(x=\"CLASE\", y=\"CLIENTE_EDAD\", data=df)\n","s.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ToTjulECPsbz"},"source":["- ¿Se observa alguna relación respecto a la edad del cliente entre aquellos que se dan de baja de los que no?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6PFKUpZPPsb0","colab":{}},"source":["# Creamos un gráfico boxplot para analizar la ANTIGÜEDAD del cliente respecto a la CLASE\n","plt.figure(figsize=(4, 8))\n","s2=sns.boxplot(x=\"CLASE\", y=\"CLIENTE_ANTIGUEDAD\", data=df)\n","s2.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ERL7qJiTPsb4"},"source":["- ¿Se observa alguna relación respecto a la antigüedad del cliente entre aquellos que se dan de baja de los que no?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ecmjSMExPsb4"},"source":["### 1.c) Análisis del estado de las tarjetas (VISA y MASTERCARD)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GbRZzV6OPsb7","colab":{}},"source":["# Estado de la tarjeta VISA\n","ct = pd.crosstab(df.VISA_CUENTA_ESTADO, df.CLASE, margins=False)   # arma una tabla cruzada\n","ct.loc[[\"NORMAL\"]].plot.barh(stacked=True)  # loc trabaja en las etiquetas del índice para armar un gráfico de barras\n","ct.loc[[\"CERRADA\"]].plot.barh(stacked=True)\n","ct.loc[[\"PROBLEMAS\"]].plot.barh(stacked=True)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2vF4m6eKPscA","colab":{}},"source":["# Estado de la tarjeta MASTERCARD\n","ct2 = pd.crosstab(df.MASTER_CUENTA_ESTADO, df.CLASE, margins=False)\n","ct2.loc[[\"NORMAL\"]].plot.barh(stacked=True)\n","ct2.loc[[\"CERRADA\"]].plot.barh(stacked=True)\n","ct2.loc[[\"PROBLEMAS\"]].plot.barh(stacked=True)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Rm5eOLTNPscG"},"source":["### Análisis preliminar\n","\n","Según se puede observar en ambas tarjetas, los clientes que tienen la cuenta \"CERRADA\" o con \"PROBLEMAS\" tienen muchísima mayor probabilidad de irse del banco."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0d8RmxYePscH"},"source":["### 1.d) Análisis del atraso en el pago de las tarjetas (VISA y MASTERCARD)\n","Los atributos que marcan el atraso en el pago de las tarjetas son VISA_MARCA_ATRASO y MASTER_MARCA_ATRASO."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tpA31IzhPscI","colab":{}},"source":["# Como ambos atributos son numericos creamos dos variables auxiliares \"categoricas\" para ver cómo se comportan\n","df['VISA_MARCA_ATRASO2'] = list(map(lambda marca: 'CON ATRASO' if (marca > 0 ) else 'SIN ATRASO', df['VISA_MARCA_ATRASO']))\n","ct3 = pd.crosstab(df.VISA_MARCA_ATRASO2, df.CLASE, margins=False)\n","ct3.loc[[\"SIN ATRASO\"]].plot.barh(stacked=True)\n","ct3.loc[[\"CON ATRASO\"]].plot.barh(stacked=True)\n","plt.show()\n","\n","# Para terminar borramos la variable auxiliar creada\n","df.drop('VISA_MARCA_ATRASO2', axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"su3lb-sxPscM","colab":{}},"source":["df['MASTER_MARCA_ATRASO2'] = list(map(lambda marca: 'CON ATRASO' if (marca > 0 ) else 'SIN ATRASO', df['MASTER_MARCA_ATRASO']))\n","ct4 = pd.crosstab(df.MASTER_MARCA_ATRASO2, df.CLASE, margins=False)\n","ct4.loc[[\"SIN ATRASO\"]].plot.barh(stacked=True)\n","ct4.loc[[\"CON ATRASO\"]].plot.barh(stacked=True)\n","plt.show()\n","\n","# Para terminar borramos la variable auxiliar creada\n","df.drop('MASTER_MARCA_ATRASO2', axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0xODFQvkPscP"},"source":["- ¿Qué puede decirse del análisis realizado?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ew9DvUP1WFN8"},"source":["## 2. Preparación de datos\n","\n","Una vez cargados los datos y analizados, se deben preparar para ser procesados. Los 3 pasos generales que se deben seguir son:\n","\n","1. Tratamiento de outliers (no se aplica en modelos con árboles de decisión)\n","2. Feature engineering \n","3. Tratamiento de missing values (o valores nulos)\n","\n","Para el paso de feature engineering se podría usar la libreria \"featuretools\" la documentación se puede encontrar en:\n","https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html\n","\n","Con esta librería se generarán automáticamente las combinaciones \"primitivas\" posibles entre las variables que tenemos. Como ejemplo, la división de una variable con otra, el máximo, minimo, mediana, etc.\n","\n","Vamos a crear una función llamada **preparacion_de_datos(dataset)** que incluirá todas las tareas de preparación de datos necesarias para construir nuestro modelo predictivo. Esta función la vamos a utilizar tanto para el dataset de entrenamiento como para el de prueba.\n","\n","****A partir de este momento, la variable CLASE (objetivo) deja de ser una etiqueta para convertirse en una variable numerica, con valores: 0 (CONTINUAN) 1 (BAJA)****"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5EOGactuWh2x","colab":{}},"source":["# FUNCION preparacion_de_datos (parametro_entrada)\n","\n","def preparacion_de_datos(df_e):\n","  #Comenzamos haciendo una copia del dataset que la función recibe como parámetro de entrada\n","  df_s = copy.copy(df_e)\n","\n","  # Convertimos la variable clase en un numerico con 0 o 1\n","  df_s['CLASE'] = list(map(lambda clase: 1 if (clase == 'BAJA') else 0, df_s['CLASE']))\n","  \n","  # FEATURE ENGINEERING \n","  # Vamos a crear 3 atributos nuevos a partir de los datos que ya tenemos, generando otras características de los clientes.\n","  # Cada uno puede crear los atributos que considere necesario (y mejoren la predicción)\n","    \n","  # Los atributos que vamos a crear son:\n","\n","  ## 1) Cociente entre el saldo de la tarjeta y el limite, para saber qué % del límite que tiene está usando. \n","  ## Las variables \"Cocientes\" las vamos a indicar con \"C_\")\n","\n","  df_s['C_VISA_SALDO_LIM'] = list(map(lambda saldo, limite:\n","                                      saldo/limite if (limite > 0) else 0,\n","                                      df_s['VISA_MSALDOTOTAL'],\n","                                      df_s['VISA_MLIMITECOMPRA']))\n","    \n","  ## 2) Suma del saldo en ambas tarjetas (MASTER Y VISA). Las variables \"Sumas\" las vamos a indicar con \"S_\"\n","  \n","  df_s['S_VISA_MASTER_SALDO'] = ( df['VISA_MSALDOTOTAL'] + df['MASTER_MSALDOTOTAL'] )\n","\n","\n","  ## 3) Variable conceptual del negocio para indicar si el cliente tiene alguna deuda con el banco. \n","  ## La constuirmos a partir de la cantidad de préstamos personales, prendarios e hipotecarios que tenga el cliente.\n","  ## Las variables del \"Negocio\" las vamos a indicar con \"N_\"\n","\n","  df_s['N_T_DEUDAS'] = list(map(lambda pp, pr, ph, tc:\n","                                1 if (pp > 0 or pr >0 or ph >0 or tc >0 ) \n","                                else 0, \n","                                df_s['CPRESTAMOS_PERSONALES'], \n","                                df_s['CPRESTAMOS_PRENDARIOS'],\n","                                df_s['CPRESTAMOS_HIPOTECARIOS'],\n","                                df_s['S_VISA_MASTER_SALDO']))\n","\n","  # TRATAMIENTO DE VALORES NULOS\n","  # Veamos si tenemos valores nulos o infinitos. Para simplificar el análisis, reemplazaremos ambos casos por el valor 0\n","  df_s[df_s==np.inf]=np.nan\n","  df_s.fillna(0, inplace=True)\n","\n","  return df_s"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Cei0R2OFnCdZ"},"source":["## 3. Construcción del modelo predictivo sin optimizacion de hiperparámetros\n","### 3.a) Modelo inicial \n","\n","Vamos a construir un árbol de clasificación inicial usando el dataset. Los pasos que realizaremos son:\n","\n","- Leer los datos con Pandas.\n","- Comprobar si hay valores nulos y crear todas las variables nuevas.\n","- Encodear todos los atributos categóricos como booleanos usando `pd.get_dummies`\n","- Encodear las etiquetas usando `LabelEncoder`\n","- Construir una variable Y que contenga la variable objetivo a predecir y una vector X que contenga todo el resto de variables a usar en la predicción.\n","- Dividir el dataset completo en training y testing\n","- Dividir X e y con train_test_split así:\n","        train_test_split(X, y, test_size=0.3, random_state=42)\n","- Ajustar un árbol de clasificación con `max_depth=3`\n","- Visualizar el árbol usando graphviz\n","- Calcular la importancia de los atributos\n","- Calcular y mostrar la matriz de confusión\n","- Sacar la restricción de `max_depth=3` y ver si la clasificación mejora"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AbLmSw5qnCda","colab":{}},"source":["# Ejecutamos la función de preparacion de datos. \n","# Este el primer paso que debemos realizar para tener todas las variables a utilizar.\n","\n","df = preparacion_de_datos(df)\n","df.head()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rm4tfXDtPscW","colab":{}},"source":["# Dejamos en el dataset de entrenamiento todas las variables, excepto la CLASE.\n","# Borramos también el atributo FOTO_MES, ya que tiene el mismo valor para todos los registros (en este caso).\n","\n","# Encodeamos todos los atributos categóricos como booleanos con la función pd.get_dummies (sin incluir la variable objetivo). \n","\n","X  = pd.get_dummies(df.drop(['CLASE','FOTO_MES'], axis=1))\n","atributos = X.columns\n","\n","# Encodeamos las etiquetas usando LabelEncoder\n","# Convertimos la variable objetivo en una variable booleana de valores 0 o 1 para simplificar los cálculos\n","\n","le = LabelEncoder()\n","y = le.fit_transform(df['CLASE'])\n","\n","# Dividimos el dataset en entrenamiento y prueba (70% para training y 30% para testing)\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Dividimos X e y con la funcion train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    y, \n","                                                    test_size=0.3, \n","                                                    random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bKwWYjyunCdd","colab":{}},"source":["# Ajustamos un árbol de clasificación con max_depth=3\n","treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n","treeclf.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ISt53tzYnCdg"},"source":["### 3.b) Interpretación de resultados\n","- Visualizaremos el árbol de decision generado junto con las condiciones de split y los nodos resultantes.\n","- Analizaremos cómo contribuyeron las variables usadas en la predicción.\n","- Calcularemos el error de entrenamiento y de testeo del modelo generado.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9k1G5jXbnCdi","colab":{}},"source":["# Visualizamos el árbol de decisión usando graphviz\n","dot_data = export_graphviz(treeclf, out_file=None,  \n","                feature_names=atributos,  \n","                filled=True, rounded=True,  \n","                special_characters=True)  \n","graph = pydotplus.graph_from_dot_data(dot_data)\n","Image(graph.create_png())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TJeCI6U2x9xb"},"source":["### 3.b.i) Lectura del árbol de decisión generado:\n","\n","#### - Tenemos 1873 casos de baja  y 40759 que no se dieron de baja\n","    Esto lo vemos en el primer nodo, llamado \"NODO RAIZ\"\n","    \n","#### - Cada nodo de \"SPLIT\" tiene dos posibles salidas \"True\" o \"False\".\n","    True siempre a la izquierda y False a la derecha.\n","    \n","#### - Cuanto más \"AZUL\" sea el nodo, mayor probabilidad de baja tiene. Un nodo en \"NARANJA\" indica lo contrario.\n","    Como ejemplo, el nodo hoja inferior izquierdo tiene 94,66% (195/206).\n","    \n","#### - Las posibles reglas a aplicar para recorrer el lado izquierdo del árbol, son:\n","\n","    TTARJETA_VISA <= 0.5 \n","      -> MCUENTAS_SALDO <= -9278.5\n","        -> MCUENTAS_SALDO <= -37249.615\n","          -> 195 bajas, 11 no bajas, P(clase=baja) = 94,66%, P(clase=continua) = 5,34%\n","          \n","    TTARJETA_VISA <= 0.5 \n","      -> MCUENTAS_SALDO <= -9278.5\n","        -> MCUENTAS_SALDO > -37249.615\n","          -> 80 bajas, 37 no bajas, P(clase=baja) = 68,37%, P(clase=continua) = 31,63%\n","          \n","    TTARJETA_VISA <= 0.5 \n","      -> MCUENTAS_SALDO > -9278.5\n","        -> VISA_MLIMITE_COMPRA <= 3071.25\n","          -> 235 bajas, 942 no bajas, P(clase=baja) = 20%, P(clase=continua) = 80%\n","          \n","    TTARJETA_VISA <= 0.5 \n","      -> MCUENTAS_SALDO > -9278.5\n","        -> VISA_MLIMITE_COMPRA > 3071.25\n","          -> 146 bajas, 83 no bajas, P(clase=baja) = 63,75%, P(clase=continua) = 36,25%\n","\n","    ... \n","    \n","- Completar las reglas del árbol de decisión."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v0rFWSYeDnnb","colab":{}},"source":["plt.figure(figsize=(15, 8))\n","\n","y_pred_proba = treeclf.predict_proba(X_test)[::,1]\n","fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n","auc = metrics.roc_auc_score(y_test, y_pred_proba)\n","\n","# Graficamos la curva roc del arbol\n","plt.plot(fpr,tpr,label=\"DT MAX_DEPTH:3 - AUC=\"+str(round(auc,3)))\n","plt.xlabel('% Continuan', fontsize=14)\n","plt.ylabel('% Bajas', fontsize=14)\n","plt.legend(loc=4, fontsize=12)\n","\n","# Graficamos la recta del azar\n","it = [i/100 for i in range(100)]\n","plt.plot(it,it,label=\"AZAR, AUC=0.5\",color=\"red\")\n","\n","plt.title(\"Curva ROC\", fontsize=14)\n","plt.tick_params(labelsize=12);\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4j8uHynjGXOe"},"source":["### 3.b.ii) Curva ROC\n","La curva ROC nos permite visualizar como se distribuyen los casos positivos y negativos dentro de nuestro modelo. \n","\n","En resumen, nos indica que % de casos positivos (clase target) vamos a captar si tomamos X % de negativos, esto da una clara idea del poder predictivo del algoritmo. Este valor se resume en la métrica AUC (Area Under Curve), la cual indica qué tan \"rápido\" crece la curva (incluye mayor % de positivos) conforme avanza la cantidad de negativos.\n","\n","Los **mejores algoritmos**, son aquellos que tienen curvas que **comienzan con pendientes muy grandes y crecen verticalmente muy rápido**, ya que esto indica que captan mayor cantidad de positivos para la misma cantidad de negativos.\n","\n","Si lo analizamos en el contexto del Trabajo Práctico, cuanto más rápido crece verticalmente la curva, indica que **con la misma cantidad de contactos voy a obtener mejores ganancias**. Por ejemplo, si tenemos una curva ROC que para el 20% de los clientes que continuan (negativos) captamos el 60% de los clientes que se iban a dar de baja (positivos), podríamos decidir contactar al primer 20% de los clientes ordenados por la probabilidad de baja que defina el algoritmo y esto infiere una ganancia esperada de $**9.610.300**:\n","\n","    -  20% * 58.196 (Cantidad que continuan) * -500 (costo del contacto al cliente)          =  -5.819.600\n","    -  60% * 2.707  (Cantidad de bajas)      * 9.500 (ganancia neta de retener al cliente)   =  15.429.900\n","\n","    -  Neto: $9.610.300\n","\n","Cuando se prueban distintos algoritmos, lo ideal es dibujar todas las curvas ROC en el mismo gráfico y comparar cómo se distancian unas de otras. En general, no existen grandes diferencias, pero son las pequeñas diferencias las que generan mayores ganancias.\n","\n","Para comparar el poder predictivo de los algoritmos también se utiliza la métrica AUC (Área Bajo la Curva) pero esta métrica es genérica de la predicción completa y no aporta distintas visiones, depende del corte que querramos aplicar para nuestra campaña de retención (primer 10%, 20%, 40%, etc...)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FV4Y5Y61nCdm"},"source":["### 3.b.iii) Importancia de las variables\n","\n","- La primera columna es el nombre de la variable y la segunda indica la importancia relativa a la predicción. Los valores más altos indican que esa variable tiene mayor importancia para \"dividir\" las distintas clases de la variable objetivo (en nuestro caso los clientes que se dan de baja de los que continúan).\n","\n","- Hay que tener claro que esto no implica una relación positiva, la importancia mide tanto relaciones positivas (menor cantidad de cuentas VISA más probabilidad de darse de baja) como las relaciones negativas (inversa).\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bd1hTBMbnCdn","scrolled":true,"colab":{}},"source":["# Calculamos la importancia de los atributos\n","pd.DataFrame({'Atributo':atributos,\n","              'importancia':treeclf.feature_importances_}).sort_values('importancia',\n","                                                                       ascending=False).head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7RuY6Ui8Psco"},"source":["### 3.b.iv) Matriz de Confusión del Modelo y Errores de Predicción\n","Analizando la matriz de confusión, podemos entender en qué casos el modelo acierta en la predicción y cuáles falla. En base a esto, podemos determinar las métricas de evaluación de modelos. Como ser:\n","\n","- Exactitud (Accuracy) = Total de predicciones correctas / Total de predicciones\n","- Error de Predicción = 1 - Exactitud\n","- Sensitivity = Total de positivos acertados / Total de positivos reales"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eDZaLzcInCdw","colab":{}},"source":["# Calculamos y vemos la matriz de confusión del modelo utilizando el conjunto de testing\n","y_pred = treeclf.predict(X_test)\n","conf = confusion_matrix(y_test, y_pred)\n","predicted_cols = ['pred_'+str(c) for c in le.classes_]\n","pd.DataFrame(conf, index = le.classes_, columns = predicted_cols)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G8ssMyZSPsct"},"source":["- Exactitud (Accuracy) = 17557 / 18271 = 96.1%\n","- Error de Predicción = 1 - Exactitud = 3.9%\n","- Sensitivity = 217 / 834 = 26%"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y21lYuSgnCdr"},"source":["## 4. Función de Ganancia y Probabilidad de Corte\n","\n","Vamos a definir una función de ganancia con la probabilidad de corte para el trabajo práctico.\n","\n","Cada llamada que el banco realiza a los clientes le cuesta **500 pesos**. Si el modelo acierta en la predicción de retención del cliente el banco gana **10000 pesos**, es decir **9500 pesos** netos. En cambio si no acierta, el banco pierde **500 pesos**. En base a estos datos, el modelo debe definir a qué clientes hay que contactar maximizando las ganancias obtenidas por la campaña de retención. En el documento **Notas_Trabajo_Practico.doc** se indica cómo obtener la probabilidad necesaria del modelo para las bajas de manera de maximizar la ganancia en la campaña.\n","\n","Este es un caso de uso de custom functions para scoring en Grid Searching, ya que por defecto la métrica usada para este tipo de algoritmos es AUC (Area Under the Curve) pero que en la vida real tiene poca aplicación."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BQl_upsvx2V9","colab":{}},"source":["prob_corte = float(0.05) # probabilidad de corte calculada \n","\n","#Definimos la función de ganancia a utilizar para el scoring en la busqueda:\n","def funcion_ganancia(clf, X, y_true):\n","    y_prob_baja = clf.predict_proba(X)[:, 1]\n","    \n","    ganancia = sum([(9500 if y_true[i] > 0 else -500) \n","                    if y_prob_baja[i]  > prob_corte\n","                    else 0 \n","                    for i in range(len(y_prob_baja))])\n","    \n","    return ganancia"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UJfna6qhnCd0"},"source":["## 5. Construcción del modelo usando GridSearchCV con optimización de hiperparámetros\n","\n","Las clase **GridSearchCV** se utiliza para automatizar la selección de los parámetros de un modelo, aplicando para ello la técnica de validación cruzada. Partiendo de un modelo y un conjunto de parámetros, GridSearchCV prueba múltiples combinaciones y selecciona los valores de los parámetros que ofrecen mayor rendimiento para un modelo y conjunto de datos. \n","\n","Los parámetros a optimizar son:\n","\n","Medida            | Que hace\n","------------------|-------------\n","max_depth         | limita la altura del árbol (niveles-2)\n","max_features      | limita la cantidad de atributos a utilizar en una división\n","max_leaf_nodes    | limita la cantidad máxima de nodos hoja puede tener el árbol\n","min_samples_leaf  | cantidad mínima de muestras de una hoja\n","min_samples_split | cantidad mínima de muestras para dividir un nodo\n","\n","Cada uno puede definir sus propios rangos de valores para cada parámetro, considerando que cuantos más parámetros se usen, el tiempo de procesamiento crece exponencialmente."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OKL5f87mPscx"},"source":["### 5.a) Definición de parámetros"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LNb6h3_jnCd3","colab":{}},"source":["# Definimos los parametros a evaluar:\n","\n","PARAMETROS = {'max_depth':[6, 12],\n","              'max_features':[5, 20], \n","              'max_leaf_nodes':[500], \n","              'min_samples_leaf':[1000],\n","              'min_samples_split':[2000]}\n","\n","k_n_jobs = 2 # numero de iteraciones definidas\n","\n","# Hacemos la búsqueda con GridSearchCV\n","\n","model = DecisionTreeClassifier(random_state=1)  # modelo de árbol de decision\n","gs = GridSearchCV(model, \n","                  PARAMETROS, \n","                  n_jobs=k_n_jobs,\n","                  scoring=funcion_ganancia,\n","                  cv=StratifiedKFold(n_splits=3,shuffle=True,random_state=1), #Cross Validation de 3 capas\n","                  verbose=1)\n","gs.fit(X, y)\n","\n","# Mostramos los mejores resultados obtenidos\n","\n","print(gs.best_estimator_)\n","print(gs.best_score_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XClgF3lJPsc2"},"source":["### 5.b) Interpretación de resultados\n","- Visualizaremos el árbol de decision generado con GridSearchCV  junto con las condiciones de split y los nodos resultantes.\n","- Analizaremos cómo contribuyeron las variables usadas en la predicción.\n","- Calcularemos el error de entrenamiento y de testeo del modelo generado."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N_kiH4awPsc3","colab":{}},"source":["# Visualizamos el mejor árbol de decisión generado usando graphviz\n","dot_data=export_graphviz(gs.best_estimator_, \n","                         out_file=None,\n","                         feature_names=X.columns,  \n","                         filled=True, rounded=True,\n","                         special_characters=True)  \n","graph = pydotplus.graph_from_dot_data(dot_data)\n","Image(graph.create_png())  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CsSNHVkDPsc6"},"source":["### 5.b.i) Importancia de las variables"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ap11x3u_CVoA","colab":{}},"source":["# Calculamos la importancia de los atributos\n","pd.DataFrame({'Atributo':atributos,\n","              'importancia':gs.best_estimator_.feature_importances_}).sort_values('importancia',\n","                                                                       ascending=False).head(15)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rxNXbna9Lb_-"},"source":["### 5.b.ii) Curva ROC\n","Comparamos las curvas ROC y metricas AUC de los distintos árboles generados por GridSearching, para enteneder que valores de hiperparámetros mejoran la capacidad predictiva del algoritmo y que tan grande (o pequeña) es la diferencia entre los árboles."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5Sa9CzniL4ax","colab":{}},"source":["# Como primer paso, vamos a probar todas las combinaciones de parametros\n","# y vamos a guardar los resultados de cada arbol en la lista \"resultados\"\n","resultados = []\n","# Recorremos las combinaciones de parametros y guardo el arbol entrenado, parametros y la prediccion\n","for p in gs.cv_results_['params']:\n","  treeclf2 = DecisionTreeClassifier(max_depth= p['max_depth'],\n","                                    max_features= p['max_features'],\n","                                    max_leaf_nodes= p['max_leaf_nodes'],\n","                                    min_samples_split= p['min_samples_split'],\n","                                    random_state=1)\n","  treeclf2.fit(X_train, y_train)\n","  y_pred_proba = treeclf2.predict_proba(X_test)[::,1]\n","  \n","  # Guardamos el arbol, la predicción y los parametros de cada ejecucion\n","  resultados.append({\"arbol\": copy.copy(treeclf2), \"prediccion\":copy.copy(y_pred_proba), \"parametros\":copy.copy(p)})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"poD_H-64o00K"},"source":["### Graficamos la curva ROC de cada árbol"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VfnJGJRKgaR4","colab":{}},"source":["# Graficamos la curva ROC del arbol de cada iteracion\n","def graficarCurvaRoc(arbol):\n","  # Calculamos la probabilidad predecida\n","  y_pred_proba = r['prediccion']\n","  \n","  # Calculamos los valores de la curva ROC para graficar\n","  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n","  \n","  # Calculamos el Area Bajo la Curva (AUC) y la guardo\n","  auc = metrics.roc_auc_score(y_test, y_pred_proba)\n","  r['auc'] = auc\n","\n","  # Graficamos\n","  plt.plot(fpr,tpr) #,label= \"AUC=\"+str(auc))\n","  plt.legend(loc=4, fontsize=12)\n","\n","# Inicializamos los labels del grafico\n","plt.figure(figsize=(20, 10))\n","plt.xlabel('% Continuan', fontsize=14)\n","plt.ylabel('% Bajas', fontsize=14)\n","\n","# Graficamos la recta del azar\n","it = [i/100 for i in range(100)]\n","plt.plot(it,it,label=\"AZAR, AUC=0.5\",color=\"black\")\n","\n","# Para cada arbol probado (en la variable resultados) graficamos la curva roc\n","for r in resultados:\n","    graficarCurvaRoc(r)\n","\n","# Agregamos el titulo y configuro el tamaño de letra\n","plt.title(\"Curva ROC\", fontsize=14)\n","plt.tick_params(labelsize=12);\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6eaylm6xphji"},"source":["#### Preguntas importantes: ¿Cuál es el mejor modelo? ¿Qué % de corte eligirían para contactar a sus clientes?\n","\n","### 5.b.iii) Análisis de hiperparámetros y sus rangos de valores\n","\n","Ahora podemos listar, para cada valor de los parámetros como mejora (o empeora) el Area Bajo La Curva (AUC) del modelo generado.\n","\n","***NO NECESARIAMENTE IMPLICA QUE EL MODELO ES MEJOR***."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gLfz43tEqSjx","colab":{}},"source":["# Guardamos todas las combinatorias de parametros y auc probados\n","metricas = []\n","metricasxArbol = []\n","for r in resultados:\n","  metricasxArbol.append( { 'parametros':'{MD: '+str(r['parametros']['max_depth'])+', MF: '+str(r['parametros']['max_features'])+', ML: '+str(r['parametros']['max_leaf_nodes'])+', MS: '+str(r['parametros']['min_samples_split'])+'}', 'auc': r['auc']})\n","  for p in r['parametros']:\n","    metricas.append({'tipo':p, 'valor':r['parametros'][p], 'auc': r['auc']})\n","\n","# Pasamos a dataframe pandas\n","metricas = pd.DataFrame(metricas)\n","metricasxArbol = pd.DataFrame(metricasxArbol)\n","\n","# Calculamos las funciones agrupadas con tablas cruzadas\n","ct_av = pd.crosstab(metricas['valor'], metricas['tipo'], values=metricas['auc'], aggfunc=np.average)\n","\n","# Graficamos los valores de cada parametros y la ganancia generada promedio\n","for c in ct_av.columns:\n","    plt.figure(figsize=(5, 5))\n","    plt.title('AUC Promedio en funcion del parametro '+str(c), fontsize=14)\n","    plt.plot(ct_av.dropna(subset=[c])[[c]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LQU2HeXiCZpc","colab":{}},"source":["# Por último, vemos que valores para cada parametro generan la mejor AUC.\n","metricasxArbol.sort_values('auc', ascending=False).head(20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w-gKrFwKPsc-"},"source":["### 5.b.iv) Matriz de Confusión del Modelo y Errores de Predicción"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"spJvCRrdnCd7","colab":{}},"source":["# Calculamos y vemos la matriz de confusión del modelo utilizando el conjunto de testing\n","conf = confusion_matrix(y_test, gs.best_estimator_.predict(X_test))\n","predicted_cols = ['pred_'+str(c) for c in le.classes_]\n","pd.DataFrame(conf, index = le.classes_, columns = predicted_cols)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fNMc_gLxPsdB"},"source":["- Exactitud (Accuracy) = 17437 / 18271 = 95.43%\n","- Error de Predicción = 1 - Exactitud = 3.87%\n","- Sensitivity = 199 / 834 = 23.86%"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FXrCFr1yyK5y"},"source":["## 6. Aplicando el modelo a datos nuevos\n","\n","Ahora podemos aplicar el modelo generado a los datos nuevos donde no conocemos la clase (todos se indican como CONTINUA) para predecir las futuras bajas. El set de prueba con los nuevos datos a utilizar es **dataset_apply.csv** el cual contiene información sobre 61751 clientes (personas físicas) con paquetes \"Premium\".\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CQ3xiuY2x2Wp","colab":{}},"source":["#Leemos el dataset de aplicacion, para predecir futuras bajas:\n","\n","dataset_aplicacion='dataset_apply.csv'\n","df_apply = pd.read_csv(dataset_aplicacion, sep=';', na_values='.', index_col=0)\n","\n","# Siempre el primer paso es ejecutar la función de preparacion de datos que construimos\n","df_apply = preparacion_de_datos(df_apply)\n","\n","# Borramos la CLASE y el campo FOTO_MES, ya que ambos son constantes, en este caso.\n","df_apply = df_apply.drop(['CLASE','FOTO_MES'], axis=1)\n","\n","# Veamos como quedó el dataset\n","df_apply.head(10)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Haw-t8Fxx2Wr","colab":{}},"source":["# Encodeamos todos los atributos categóricos como booleanos con la función pd.get_dummies (sin incluir la variable objetivo). \n","X_apply = pd.get_dummies(df_apply)\n","atributos = X_apply.columns\n","\n","# Aplicamos la predicción al nuevo dataset\n","scores = gs.best_estimator_.predict_proba(X_apply)\n","\n","# Agregamos al dataset la probabilidad de baja predicha con el modelo\n","df_result = copy.copy(df_apply)\n","df_result['PROB'] = scores[:,1]\n","\n","# Veamos como quedó:\n","df_result.sort_values(by='PROB', ascending=False).head(20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_1__8jJRyg4U"},"source":["## 7. Entregable Final\n","Para terminar, vamos a generar un archivo de salida \"csv\" con los clientes cuya probabilidad de baja sea mayor a la de corte que habíamos calculado."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZjwdY4PWx2W5","colab":{}},"source":["# Clientes cuya probabilidad de baja sea mayor a la probabilidad de corte (0.05)\n","\n","dataset_entrega='dataset_entrega.csv'\n","\n","df_entregar = df_result[df_result.PROB > prob_corte]\n","df_entregar.to_csv(path_or_buf=dataset_entrega, sep=\";\", na_rep='.')\n","df_entregar.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fmeHlQ8ZLs0m"},"source":["## FIN"]}]}